# -*- coding: utf-8 -*-
"""suspiciousdata.ipnyb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xmc5sg4nzoybU3xTywa2AH-l1x91r7TR

#SmartWattSecure

##Classifying Suspicious Data

###importing libraries
"""

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, classification_report, confusion_matrix, accuracy_score
from sklearn.svm import SVC
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.utils import shuffle
from sklearn.neighbors import KNeighborsClassifier

"""###loading dataset"""

def load_data_from_csv(filename, delimiter=','):
    """
    Loads data from a CSV file into a NumPy array, handling missing values.
    """
    try:
        # Assuming CSV file has a header row
        data = np.genfromtxt(filename, delimiter=delimiter, skip_header=1)
        # Impute missing values using mean strategy
        imputer = SimpleImputer(strategy='mean')
        data = imputer.fit_transform(data)
    except ValueError:
        print("Error: Could not load data from", filename)
        return None  # Indicate error by returning None

    return data


def load_data(filename, delimiter=','):
    """
    Loads data from a CSV file into a Pandas DataFrame, handling missing values.
    """
    try:
        data = pd.read_csv(filename, delimiter=delimiter)

        # Parse Date and Time with the correct format
        data['DateTime'] = pd.to_datetime(data['Date'] + ' ' + data['Time'], format='%d/%m/%Y %H:%M:%S')

        # Drop the original Date and Time columns
        data = data.drop(['Date', 'Time'], axis=1)

        # Optionally, set DateTime as index
        data.set_index('DateTime', inplace=True)

        # Convert relevant columns to numeric
        data['Global_active_power'] = pd.to_numeric(data['Global_active_power'], errors='coerce')
        data['Global_reactive_power'] = pd.to_numeric(data['Global_reactive_power'], errors='coerce')
        data['Voltage'] = pd.to_numeric(data['Voltage'], errors='coerce')
        data['Global_intensity'] = pd.to_numeric(data['Global_intensity'], errors='coerce')
        data['Sub_metering_1'] = pd.to_numeric(data['Sub_metering_1'], errors='coerce')
        data['Sub_metering_2'] = pd.to_numeric(data['Sub_metering_2'], errors='coerce')

        # Impute missing values using mean strategy
        imputer = SimpleImputer(strategy='mean')
        data.iloc[:, :] = imputer.fit_transform(data)

    except ValueError as e:
        print("Error: Could not load data from", filename)
        print(e)
        return None  # Indicate error by returning None

    return data

"""###suspicious data function"""

def suspicious_data_selection(filename):

    X= load_data_from_csv("household_power_consumption.csv", delimiter=',')

    kmeans = KMeans(n_clusters=2, random_state=0)
    cluster_labels = kmeans.fit_predict(X)

    CN = X[cluster_labels == 0]
    CU = X[cluster_labels == 1]
    print("Number of Data Points in CN (Normal Class):", len(CN))
    print("Number of Data Points in CU (Suspicious Class):", len(CU))

    scaler = StandardScaler()
    X_standardized = scaler.fit_transform(X)

    scaler.fit(CN)

    x_bar = scaler.mean_
    sigma = scaler.scale_
    sigma_squared = sigma * sigma

    print("Mean of CN:", x_bar)
    print("Standard Variance of CN:", sigma_squared)


    std_dev = np.std(X)
    std_dev = std_dev/1000
    print("Standard Deviation of Overall Dataset:", std_dev)

    threshold1 =  0.05*std_dev +0.5
    print("Threshold (Based on Overall Standard Deviation):", threshold1)


    mean_CU = np.mean(CU)
    std_dev_CU = np.std(CU)

    z_scores = (CU - mean_CU) / std_dev_CU
    print("Mean of CU:", mean_CU)
    print("Standard Variance of CU:", std_dev_CU)
    print("Z-Scores of CU:", z_scores)
    return CU, CN

"""##Calculating SCs"""

def sc_score(X, max_clusters=11):
    X = shuffle(X)

    batch_size = 10000
    scaler = StandardScaler()

    cluster_scores = {}
    average_scores = {}

    for num_clusters in range(3, max_clusters + 1):
        cluster_number = num_clusters - 2
        cluster_scores[cluster_number] = []

        kmeans = KMeans(n_clusters=num_clusters, random_state=0)
        kmeans.fit(scaler.fit_transform(X))

        batch_scores = []

        for i in range(0, len(X), batch_size):
            batch_X = X[i:i + batch_size]
            scaled_batch_X = scaler.transform(batch_X)
            cluster_labels = kmeans.predict(scaled_batch_X)
            score = silhouette_score(scaled_batch_X, cluster_labels)
            batch_scores.append(score)

        average_score = sum(batch_scores) / len(batch_scores)
        print(f"Average silhouette score for {cluster_number} cluster: {average_score}")

        cluster_scores[num_clusters] = batch_scores
        average_scores[num_clusters] = average_score
    return cluster_scores, average_scores


def highest_score_above_threshold(average_scores, threshold):
    highest_score = float('-inf')
    best_key = None
    for key, score in average_scores.items():
        if score > threshold and score > highest_score:
            highest_score = score
            best_key = key
    return best_key, highest_score


def semi_svm_based_pattern_matching(Csuspicious, L, C, y, thresholdG=-1.00):
    def test_point_generation(x_bar, De, Dc):
        return 0.5 * (De + Dc)

    def zscore(x, x_bar, sigma):
        return (x - x_bar) / sigma

    def knn(k, x, L):
        knn_model = KNeighborsClassifier(n_neighbors=k)
        knn_model.fit(L[:, :-1], L[:, -1])
        return knn_model.predict([x[:-1]])[0]

    def svm(L, y):
        svm_model = SVC(kernel='linear', C=1.0, random_state=42)
        svm_model.fit(L[:, :-1], y)
        return svm_model

    def classification_loss(y_true, y_pred):
        return mean_squared_error(y_true, y_pred)

    Ccandidate = Csuspicious
    Canomaly = []

    y_train_local = y.copy()
    for i in range(len(C)):
        Ci = C[i]
        x_bar_i = np.mean(Ci, axis=0)
        sigma_i = np.std(Ci, axis=0)

        # Print debugging information
        print(f"Cluster {i + 1}: Size of Ci = {len(Ci)}")
        print(f"x_bar_i: {x_bar_i}, sigma_i: {sigma_i}")

        if len(Ci) > 0:
            z_scores = np.array([zscore(xc, x_bar_i, sigma_i) for xc in Ci])
            print(f"z_scores: {z_scores}")

            De_index = np.argmax(z_scores) if len(z_scores) > 0 else None
            Dc_index = np.argmin(z_scores) if len(z_scores) > 0 else None
            print(f"De_index: {De_index}, Dc_index: {Dc_index}")

            if De_index is not None and Dc_index is not None:
                De_i = Ci[De_index]
                Dc_i = Ci[Dc_index]
                print(f"De_i: {De_i}, Dc_i: {Dc_i}")

                Dt_i = test_point_generation(x_bar_i, De_i, Dc_i)
                print(f"Dt_i: {Dt_i}")

                Li = knn(1, Dt_i, L)
                print(f"Li: {Li}")

                svm_model = svm(L, y_train_local)
                y_pred_before = svm_model.predict(L[:, :-1])
                loss1 = classification_loss(y_train_local, y_pred_before)
                print(f"loss1: {loss1}")

                L = np.vstack([L, Dt_i])
                y_train_local = np.append(y_train_local, Li)
                print(f"L updated: {L.shape}, y_train_local updated: {y_train_local.shape}")

                svm_model = svm(L, y_train_local)
                y_pred_after = svm_model.predict(L[:, :-1])
                loss2 = classification_loss(y_train_local, y_pred_after)
                print(f"loss2: {loss2}")

                if (loss2 - loss1) < thresholdG:
                    print(f"Cluster {i + 1} is not anomalous")
                    Ccandidate = [x for x in Ccandidate if x not in Ci]
                else:
                    print(f"Cluster {i + 1} is anomalous")
                    Canomaly.extend([x for x in Ccandidate if x in Ci])
            else:
                print(f"Warning: Could not determine De_index or Dc_index for cluster {i + 1}")
        else:
            print(f"Warning: Empty cluster Ci detected.")

    return Canomaly

# Load and preprocess the data for SVM training
data = load_data('household_power_consumption.csv', delimiter=',')
if data is not None:
    # Create target variable: 0 for normal (<= 3kWh), 1 for suspicious (> 3kWh)
    data['Target'] = (data['Global_active_power'] > 3).astype(int)

    # Select features and target variable
    X = data[['Global_active_power', 'Global_reactive_power', 'Voltage',
              'Global_intensity', 'Sub_metering_1', 'Sub_metering_2','Sub_metering_3']]
    y = data['Target']

    # Standardize the data
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

    # Initialize the SVM classifier
    svm_model = SVC(kernel='linear', C=1.0, random_state=42)

    # Train the model
    svm_model.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = svm_model.predict(X_test)

    # Evaluate the model
    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))
    print(f"Accuracy: {accuracy_score(y_test, y_pred)}")

    # Predict on the entire dataset
    data['Predicted_Target'] = svm_model.predict(X_scaled)

    # Separate normal and suspicious data points
    normal_data = data[data['Predicted_Target'] == 0]
    suspicious_data = data[data['Predicted_Target'] == 1]

    # Display some of the suspicious data points
    print(suspicious_data.head())
    # Apply semi-SVM based pattern matching on suspicious clusters
    L = np.column_stack((X_train, y_train))
    C = [np.column_stack((X_train[y_train == label], y_train[y_train == label])) for label in np.unique(y_train)]
    Csus, Cnor = suspicious_data_selection('household_power_consumption.csv')
    Canomaly = semi_svm_based_pattern_matching(Csus, L, C, y_train)



    print("Anomalous Data Points Identified:")
    print(Canomaly)
else:
    print("Data loading failed.")